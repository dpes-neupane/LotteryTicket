{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scotty/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn, Tensor\n",
    "from typing import Union, Tuple\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "from model import Model \n",
    "# import math\n",
    "import pickle\n",
    "\n",
    "seed_number = 42\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "random.seed(seed_number)\n",
    "torch.manual_seed(seed_number)\n",
    "torch.cuda.manual_seed(seed_number)\n",
    "np.random.seed(seed_number)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_number)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 64 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadData(batch_size, download=True):\n",
    "\n",
    "\n",
    "    transforms = v2.Compose(\n",
    "        [\n",
    "            v2.ToImage(), \n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Grayscale(1),\n",
    "            v2.Normalize((0.5, ), (0.5, ))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST('./', train=True, transform=transforms, download=download)\n",
    "    train_subset, val_subset = torch.utils.data.random_split(\n",
    "        trainset, [0.9, 0.1], generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    testset = torchvision.datasets.MNIST('./', train=False, transform=transforms, download=download)\n",
    "        \n",
    "    train_data = torch.utils.data.DataLoader(train_subset,  batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_data = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    val_data = torch.utils.data.DataLoader(val_subset,  batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.show()\n",
    "\n",
    "def calculateAccuracy(predicted, targets):\n",
    "    predicted = nn.functional.softmax(predicted, dim=0)    # print(predicted[0], targets[0])\n",
    "    pred_no = torch.argmax(predicted, dim=1)\n",
    "    # print(predicted)\n",
    "    right = torch.sum(torch.eq(pred_no, targets).int())\n",
    "    return right / len(pred_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateModel(model, testIter, loss, device):\n",
    "    loss_per_batch = []\n",
    "    acc_per_batch = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for _, (X, y) in enumerate(testIter):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            out = model(X)\n",
    "            l = loss(out, y)\n",
    "            a = calculateAccuracy(out, y)\n",
    "            loss_per_batch.append(l.item())\n",
    "            acc_per_batch.append(a.item())\n",
    "        mean_acc = sum(acc_per_batch) / len(acc_per_batch)\n",
    "        meanloss = sum(loss_per_batch)/len(loss_per_batch)\n",
    "    return loss_per_batch, meanloss, acc_per_batch, mean_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainIter, testIter, model,\n",
    "          device=device,\n",
    "          epochs=100, \n",
    "          optim=None, \n",
    "          loss=None,\n",
    "          scheduler=None\n",
    "          ):\n",
    "    logs_dic = {\n",
    "        \"valildationLoss\": [],\n",
    "        \"trainingLoss\" : [],\n",
    "        \"validationAccuracy\": [],\n",
    "        \"trainingAccuracy\": []\n",
    "    }\n",
    "    for epoch in range(epochs):\n",
    "        train_loss_per_batch = []\n",
    "        train_acc_per_batch = []\n",
    "        with tqdm(trainIter, unit=\"batches\") as tepoch:\n",
    "            for _, (X, y) in enumerate(tepoch):\n",
    "                model.train()\n",
    "                optim.zero_grad()\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                out = model(X)\n",
    "                l = loss(out, y)\n",
    "                acc = calculateAccuracy(out, y)\n",
    "                train_acc_per_batch.append(acc.item())\n",
    "                train_loss_per_batch.append(l.item())\n",
    "                tepoch.set_description(f\"Epoch {epoch + 1}\")\n",
    "                tepoch.set_postfix(loss=l.item(), accuracy=acc.item())\n",
    "                l.backward()\n",
    "                optim.step()\n",
    "        val_loss, mean_val_loss, val_acc, mean_val_acc = validateModel(model, testIter, loss=loss, device=device)\n",
    "        print(f\"The validation loss is: {mean_val_loss}\")\n",
    "        print(f\"The validation accuracy is: {mean_val_acc}\")\n",
    "        logs_dic['valildationLoss'].append(val_loss)\n",
    "        logs_dic['trainingLoss'].append(train_loss_per_batch)\n",
    "        logs_dic['trainingAccuracy'].append(train_acc_per_batch)\n",
    "        logs_dic['validationAccuracy'].append(val_acc)\n",
    "        if scheduler: \n",
    "            if scheduler.__module__ == lr_scheduler.__name__:\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                for param_group in optim.param_groups:\n",
    "                    lr = scheduler(epoch)\n",
    "                    param_group['lr'] = lr\n",
    "\n",
    "    return logs_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(testIter, model, device=device):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        acc_batch = []\n",
    "        for _, (X, y) in enumerate(tqdm(testIter)):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            out = model(X)\n",
    "            a = calculateAccuracy(out, y)\n",
    "            acc_batch.append(a.item())\n",
    "    total_acc = sum(acc_batch) / len(acc_batch)\n",
    "    return total_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainloader, valloader, testloader = downloadData(BATCH_SIZE, download=False)\n",
    "# model = Model()\n",
    "# model.load_state_dict(torch.load(\"base.pth\", map_location=device))\n",
    "# # optim = optim.Adam(model.parameters(), lr=0.0001 )\n",
    "# optim = optim.SGD(model.parameters(), lr=0.009, momentum=0.93, dampening=0.05, weight_decay=0.009)\n",
    "# # optim = optim.SGD(model.parameters(), lr=0.0023)\n",
    "# acc = test(testloader, model)\n",
    "# loss = nn.CrossEntropyLoss()\n",
    "# print(\"The test accuracy is: \", acc)\n",
    "# logs = train(trainloader, valloader, model, \n",
    "#                 device=device, epochs=20, optim=optim, loss=loss)\n",
    "# acc = test(testloader, model)\n",
    "# print(\"The test accuracy is: \", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskWeights(weight_matrix, prune_rate):\n",
    "    mask = torch.ones_like(weight_matrix)\n",
    "    mask = mask * (weight_matrix!=0)\n",
    "    prev_l, curr_l = weight_matrix.shape\n",
    "    abs_wght_matrix = torch.abs(weight_matrix)\n",
    "    sorted_tensor, sorted_indices = torch.sort(abs_wght_matrix.reshape(1, -1))\n",
    "    non_zeros = sorted_tensor.nonzero()\n",
    "    # print(sorted_tensor, non_zeros)\n",
    "    first_non_zero_index = non_zeros[0, 1]\n",
    "    amt_pruning = torch.floor((len(torch.squeeze(sorted_tensor)) - first_non_zero_index) * float(prune_rate/100))\n",
    "    # print(first_non_zero_index + amt_pruning.int())\n",
    "    p_indices = sorted_indices[0][first_non_zero_index: first_non_zero_index+(amt_pruning.int())]\n",
    "    # print(p_indices.shape)\n",
    "    # convert to the original indices\n",
    "    izz, jzz = p_indices // curr_l, p_indices % curr_l\n",
    "    mask[izz, jzz] = 0\n",
    "    # print(weight_matrix)\n",
    "    return mask\n",
    "    \n",
    "def saveMasks(masks, filename):\n",
    "    print(\"pickling masks.\")\n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(masks, fp)\n",
    "    print(\"successfully pickled\") \n",
    "\n",
    "def retrieveMasks(filename):\n",
    "    print(\"unpickling masks.\")\n",
    "    with open(filename, 'rb') as fp:\n",
    "        masks = pickle.load( fp)\n",
    "    print(\"successfully unpickled\") \n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freezeGrads(mask):\n",
    "    def callback(module, grad_input, grad_output):\n",
    "        module.weight.grad.data = module.weight.grad.data * mask\n",
    "    return callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterativePruning(trainIter=None, \n",
    "                    model=None, \n",
    "                    masks=None,\n",
    "                    prune_rate=None,\n",
    "                    device=device,\n",
    "                    optim=None, \n",
    "                    loss=None,\n",
    "                    ):\n",
    "    logs_dic = {\n",
    "        # \"valildationLoss\": [],\n",
    "        \"trainingLoss\" : [],\n",
    "        # \"validationAccuracy\": [],\n",
    "        \"trainingAccuracy\": []\n",
    "    }\n",
    "    # for epoch in range(epochs):\n",
    "    #     train_loss_per_batch = []\n",
    "    #     train_acc_per_batch = []\n",
    "    # with tqdm(trainIter, unit=\"batches\") as tepoch:\n",
    "    for _, (X, y) in enumerate(trainIter):\n",
    "        model.train()\n",
    "        optim.zero_grad()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        X.requires_grad = True\n",
    "        out = model(X)\n",
    "        l = loss(out, y)\n",
    "        acc = calculateAccuracy(out, y)\n",
    "        print(\"The training loss is: \", l)\n",
    "        print(\"The training accuracy is: \", acc)\n",
    "        \n",
    "        # train_acc_per_batch.append(acc.item())\n",
    "        # train_loss_per_batch.append(l.item())\n",
    "        # tepoch.set_description(f\"Epoch {epoch + 1}\")\n",
    "        # tepoch.set_postfix(loss=l.item(), accuracy=acc.item())\n",
    "        hook1 = model.layer1.register_full_backward_hook(freezeGrads(masks[0]))\n",
    "        hook2 = model.layer1.register_full_backward_hook(freezeGrads(masks[1]))\n",
    "        hook3 = model.layer2.register_full_backward_hook(freezeGrads(masks[2]))\n",
    "        hook4 = model.layer3.register_full_backward_hook(freezeGrads(masks[3]))\n",
    "        hook5 = model.outputLayer.register_full_backward_hook(freezeGrads(masks[4]))\n",
    "        \n",
    "        l.backward()\n",
    "        optim.step()\n",
    "        mask1 = maskWeights(model.layer1.weight, prune_rate)\n",
    "        mask2 = maskWeights(model.layer2.weight, prune_rate)\n",
    "        mask3 = maskWeights(model.layer3.weight, prune_rate)\n",
    "        mask4 = maskWeights(model.layer4.weight, prune_rate)  \n",
    "        mask5 = maskWeights(model.outputLayer.weight, prune_rate/4)   \n",
    "        hook1.remove()\n",
    "        hook2.remove()\n",
    "        hook3.remove()\n",
    "        hook4.remove()\n",
    "        hook5.remove()\n",
    "        saveMasks([mask1, mask2, mask3, mask4, mask5], \"./masks/\" + str(prune_rate) +\"percentMasks\")\n",
    "        # prune_rate = prune_rate * \n",
    "        # print(mask1)\n",
    "        # save the masks\n",
    "        \n",
    "        \n",
    "        break\n",
    "    return 0\n",
    "        # val_loss, mean_val_loss, val_acc, mean_val_acc = validateModel(model, testIter, loss=loss, device=device)\n",
    "        # print(f\"The validation loss is: {mean_val_loss}\")\n",
    "        # print(f\"The validation accuracy is: {mean_val_acc}\")\n",
    "        # logs_dic['valildationLoss'].append(val_loss)\n",
    "        # logs_dic['trainingLoss'].append(train_loss_per_batch)\n",
    "        # logs_dic['trainingAccuracy'].append(train_acc_per_batch)\n",
    "        # logs_dic['validationAccuracy'].append(val_acc)\n",
    "        # if scheduler: \n",
    "        #     if scheduler.__module__ == lr_scheduler.__name__:\n",
    "        #         scheduler.step()\n",
    "        #     else:\n",
    "        #         for param_group in optim.param_groups:\n",
    "        #             lr = scheduler(epoch)\n",
    "        #             param_group['lr'] = lr\n",
    "\n",
    "    # return logs_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optimi\n",
    "\n",
    "# model = Model()\n",
    "# torch.save(model.state_dict(), 'base.pth')\n",
    "trainloader, valloader, testloader = downloadData(BATCH_SIZE, download=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pruneIterations():\n",
    "    model = Model()\n",
    "    model.load_state_dict(torch.load('base.pth', map_location='cpu'))\n",
    "    optim = optimi.SGD(model.parameters(), lr=0.009)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    # mask1 = torch.ones_like(model.layer1.weight)\n",
    "    # mask2 = torch.ones_like(model.layer2.weight)\n",
    "    # mask3 = torch.ones_like(model.layer3.weight)\n",
    "    # mask4 = torch.ones_like(model.layer4.weight)\n",
    "    # mask5 = torch.ones_like(model.outputLayer.weight)\n",
    "    \n",
    "    # iterativePruning(trainIter=trainloader,  model=model, masks=[mask1, mask2, mask3, mask4, mask5], prune_rate=3, device=device, optim=optim, loss=loss)\n",
    "    for i in range(90, 100, 1):\n",
    "        print(i)\n",
    "        masks = retrieveMasks('masks/' + str(i-1) + 'percentMasks')\n",
    "    # apply masks to weights of each layer\n",
    "        model = Model()\n",
    "        model.load_state_dict(torch.load(\"base.pth\", map_location='cpu'))\n",
    "        model.layer1.weight.data = model.layer1.weight.data * masks[0]\n",
    "        # print(model.layer1.weight.data)\n",
    "        model.layer2.weight.data = model.layer2.weight.data * masks[1]\n",
    "        model.layer3.weight.data = model.layer3.weight.data * masks[2]\n",
    "        model.layer4.weight.data = model.layer4.weight.data * masks[3]\n",
    "        model.outputLayer.weight.data = model.outputLayer.weight.data * masks[4]\n",
    "        \n",
    "        optim = optimi.SGD(model.parameters(), lr=0.09)\n",
    "        # print(device)\n",
    "        iterativePruning(trainIter=trainloader,  model=model, masks=masks, prune_rate=i, device=device, optim=optim, loss=loss)\n",
    "     \n",
    "# pruneIterations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainPrunedModels(model, masks, trainIter, testIter, epochs, loss, optim ):\n",
    "    logs_dic = {\n",
    "        \"valildationLoss\": [],\n",
    "        \"trainingLoss\" : [],\n",
    "        \"validationAccuracy\": [],\n",
    "        \"trainingAccuracy\": []\n",
    "    }\n",
    "    model.layer1.weight.data = model.layer1.weight.data * masks[0]\n",
    "    model.layer2.weight.data = model.layer2.weight.data * masks[1]\n",
    "    model.layer3.weight.data = model.layer3.weight.data * masks[2]\n",
    "    model.layer4.weight.data = model.layer4.weight.data * masks[3]\n",
    "    model.outputLayer.weight.data = model.outputLayer.weight.data * masks[4]\n",
    "    \n",
    "    hook1 = model.layer1.register_full_backward_hook(freezeGrads(masks[0]))\n",
    "    hook2 = model.layer2.register_full_backward_hook(freezeGrads(masks[1]))\n",
    "    hook3 = model.layer3.register_full_backward_hook(freezeGrads(masks[2]))\n",
    "    hook4 = model.layer4.register_full_backward_hook(freezeGrads(masks[3]))\n",
    "    hook5 = model.outputLayer.register_full_backward_hook(freezeGrads(masks[4]))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss_per_batch = []\n",
    "        train_acc_per_batch = []\n",
    "        with tqdm(trainIter, unit=\"batches\") as tepoch:\n",
    "            for _, (X, y) in enumerate(tepoch):\n",
    "                model.train()\n",
    "                optim.zero_grad()\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                X.requires_grad=True\n",
    "                out = model(X)\n",
    "                l = loss(out, y)\n",
    "                acc = calculateAccuracy(out, y)\n",
    "                train_acc_per_batch.append(acc.item())\n",
    "                train_loss_per_batch.append(l.item())\n",
    "                tepoch.set_description(f\"Epoch {epoch + 1}\")\n",
    "                tepoch.set_postfix(loss=l.item(), accuracy=acc.item())\n",
    "                l.backward()\n",
    "                \n",
    "                optim.step()\n",
    "        val_loss, mean_val_loss, val_acc, mean_val_acc = validateModel(model, testIter, loss=loss, device=device)\n",
    "        print(f\"The validation loss is: {mean_val_loss}\")\n",
    "        print(f\"The validation accuracy is: {mean_val_acc}\")\n",
    "        logs_dic['valildationLoss'].append(val_loss)\n",
    "        logs_dic['trainingLoss'].append(train_loss_per_batch)\n",
    "        logs_dic['trainingAccuracy'].append(train_acc_per_batch)\n",
    "        logs_dic['validationAccuracy'].append(val_acc)\n",
    "    hook1.remove()\n",
    "    hook2.remove()\n",
    "    hook3.remove()\n",
    "    hook4.remove()\n",
    "    hook5.remove()\n",
    "    return logs_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpickling masks.\n",
      "successfully unpickled\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks = retrieveMasks('masks/96percentMasks')\n",
    "# apply masks to weights of each layer\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load(\"base.pth\", map_location='cpu'))\n",
    "# model.layer1.weight.data = model.layer1.weight.data * masks[0]\n",
    "# # print(model.layer1.weight.data)\n",
    "# model.layer2.weight.data = model.layer2.weight.data * masks[1]\n",
    "# model.layer3.weight.data = model.layer3.weight.data * masks[2]\n",
    "# model.layer4.weight.data = model.layer4.weight.data * masks[3]\n",
    "# model.outputLayer.weight.data = model.outputLayer.weight.data * masks[4]\n",
    "\n",
    "\n",
    "# optim = optimi.SGD(model.parameters(), lr=0.009)\n",
    "# iterativePruning(trainloader, valloader, model, 60, device, optim, loss)\n",
    "\n",
    "# print(model.outputLayer.weight.data)\n",
    "# torch.sum(model.outputLayer.weight.data == torch.zeros_like(model.outputLayer.weight))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 844/844 [00:19<00:00, 43.87batches/s, accuracy=0.646, loss=0.864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation loss is: 0.8534586531050662\n",
      "The validation accuracy is: 0.5295877659574468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 844/844 [00:19<00:00, 44.02batches/s, accuracy=0.5, loss=0.477]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation loss is: 0.5131179236985267\n",
      "The validation accuracy is: 0.5938608158142009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 844/844 [00:18<00:00, 44.58batches/s, accuracy=0.708, loss=0.456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation loss is: 0.4038897573630861\n",
      "The validation accuracy is: 0.6555851063829787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 844/844 [00:19<00:00, 44.36batches/s, accuracy=0.75, loss=0.367] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation loss is: 0.3671668024456247\n",
      "The validation accuracy is: 0.6798537234042553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 844/844 [00:18<00:00, 44.46batches/s, accuracy=0.625, loss=0.471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation loss is: 0.42395737006309187\n",
      "The validation accuracy is: 0.706283244680851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 844/844 [00:18<00:00, 44.46batches/s, accuracy=0.854, loss=0.349] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation loss is: 0.3807481036699833\n",
      "The validation accuracy is: 0.7146498224836715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 844/844 [00:19<00:00, 44.38batches/s, accuracy=0.875, loss=0.478] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation loss is: 0.31993414112862123\n",
      "The validation accuracy is: 0.7511081562397328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 844/844 [00:18<00:00, 44.51batches/s, accuracy=0.729, loss=0.219] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation loss is: 0.2903837932551161\n",
      "The validation accuracy is: 0.7524933510638298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 844/844 [00:19<00:00, 44.32batches/s, accuracy=0.708, loss=0.21]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation loss is: 0.3079137896445203\n",
      "The validation accuracy is: 0.7420766841857991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 844/844 [00:19<00:00, 43.98batches/s, accuracy=0.75, loss=0.237]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation loss is: 0.29596637720440294\n",
      "The validation accuracy is: 0.7386414009205838\n"
     ]
    }
   ],
   "source": [
    "optim = optimi.SGD(model.parameters(), lr=0.01, momentum=0.94)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "logs = trainPrunedModels(model, masks, trainloader, valloader, epochs=10, loss=loss, optim=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs['lr'] = 0.01\n",
    "logs['epochs'] = 10\n",
    "logs['momentum'] = 0.94\n",
    "# logs['dampening'] = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved Successfully\n"
     ]
    }
   ],
   "source": [
    "def saveModel(model, filename):\n",
    "    torch.save(model.state_dict(), filename + \".pth\")\n",
    "    print(\"saved Successfully\")\n",
    "    \n",
    "def saveLogs(logs, filename):\n",
    "    print(\"pickling masks.\")\n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(logs, fp)\n",
    "    print(\"successfully pickled\") \n",
    "    \n",
    "saveModel(model, \"./prunedModels/96percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickling masks.\n",
      "successfully pickled\n"
     ]
    }
   ],
   "source": [
    "saveLogs(logs, 'logs/96percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
